\documentclass[10pt]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{framed}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage[capitalise,noabbrev]{cleveref}
\usepackage[usenames,dvipsnames]{xcolor}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[letterpaper, margin=1in]{geometry}
\sloppy


\renewcommand{\familydefault}{ppl}

\newcommand{\bx}{{\boldsymbol x}}
\newcommand{\bX}{\mathbf {X}}
\newcommand{\bz}{{\boldsymbol z}}
\newcommand{\bh}{{\boldsymbol h}}
\newcommand{\bw}{{\boldsymbol w}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\bmu}{{\boldsymbol \mu}}
\newcommand{\balpha}{{\boldsymbol \alpha}}
\newcommand{\bbeta}{{\boldsymbol \beta}}
\newcommand{\bSigma}{{\boldsymbol \Sigma}}
\newcommand{\btheta}{{\boldsymbol \theta}}
\newcommand{\reals}{{\mathbb R}}
\DeclareMathOperator*{\E}{{\mathbb E}}
\DeclareMathOperator*{\normal}{{\cal N}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\AND}{AND}
\DeclareMathOperator*{\OR}{OR}
\DeclareMathOperator*{\XOR}{XOR}
\DeclareMathOperator*{\sign}{sign}


\newcommand{\solution}[1]{{\color{PineGreen} \textbf{Solution:} #1}}

\newcommand{\todo}[1]{\textbf{\textcolor{MidnightBlue}{to do: #1}} }

\begin{document}

\section*{Machine Learning Homework 4}

\section*{General Instructions}

Homework must be submitted electronically on Canvas. Make sure to explain your reasoning or show your derivations. Except for answers that are especially straightforward, you will lose points for unjustified answers, even if they are correct. 

\section*{General Instructions}

You are allowed to work with at most one other student on the homework. With your partner, you will submit only one copy, and you will share the grade that your submission receives. You should set up your partnership on Canvas as a two-person group by joining one of the preset groups named ``HW4 Group $n$'' for some number $n$.

Submit your homework electronically on Canvas. We recommend using LaTeX, especially for the written problems. But you are welcome to use anything as long as it is neat and readable. 

For the programming portion, you should only need to modify the Python files. You may modify the iPython notebooks, but you will not be submitting them, so your code must work with our provided iPython notebook.

Relatedly, cite all outside sources of information and ideas. 

\section*{Written Problems}

\begin{enumerate}

\item We will derive the expectation-maximization (EM) algorithm using \textit{variational} analysis. 

Let $X = \{\bx_1, \bx_2, \ldots, \bx_n\}$ be a set of data vectors. Let $Z = \{z_1, \ldots, z_n\}$ be set of multinomial variables corresponding to which of $K$ Gaussians generated each example. Let the Gaussian parameters be means $\{\bmu_1, \ldots, \bmu_K\}$ and covariance matrices $\{\bSigma_1, \ldots, \bSigma_K\}$. Let $\Theta = \{\theta_1, \ldots, \theta_K\}$ be multinomial prior probabilities of which Gaussian generates each example.

Each data point is generated by first sampling a Gaussian index from $p(z | \Theta)$, then sampling from the Gaussian $\normal(\bmu_z, \bSigma_z)$. The log likelihood of any observations given these mixture model parameters is
\begin{align}
L(X, \bmu, \bSigma, \Theta) &= \sum_{i=1}^n \log \left(\sum_{k = 1}^K p(z_i | \Theta) \normal(x_i | \bmu_k, \Sigma_k) \right)
= \sum_{i=1}^n \log \left(\sum_{k = 1}^K \theta_k \normal(x_i | \bmu_k, \Sigma_k) \right) ~.
\label{eq:likelihood}
\end{align}

Since we will never observe any $z$ variables, these are considered \textit{hidden} or \textit{latent} variables. When computing the likelihood of observed variables, we sum over all possible states of the latent variables, weighted by the probability of those states.

We start by doing something a little weird. We create an independent distribution $q$ for the latent variables such that
\begin{align}
q(z_1, \ldots, z_n) &:= q(z_1) q(z_2) \ldots q(z_n).
\end{align}
We then rewrite the log likelihood from \cref{eq:likelihood} so that each data point's likelihood is multiplied by the $q$ distribution divided by itself. In other words, we multiply the terms in the innermost summation by $\frac{q(z_i = k)}{q(z_i = k)}$, resulting in the equivalent form of the likelihood:
\begin{align}
L(X, \bmu, \bSigma, \Theta, q) =& \sum_{i=1}^n \log \left(\sum_{k = 1}^K \theta_k \normal(x_i | \bmu_k, \Sigma_k) \underbrace{q(z_i = k) / q(z_i = k)}_{1} \right) .
\end{align}

Jensen's inequality guarantees that for any convex function $\varphi$ and any distribution over random variable $X$,
\begin{align}
\varphi\left( \E \left[ X \right] \right) \le \E \left[  \varphi(X) \right]~.
\label{eq:jensen}
\end{align}
We use Jensen's inequality and the fact that $\log$ is a \textbf{concave} function (i.e., $-\log$ is a convex function)  to form a lower bound on the log likelihood:
\begin{align}
L(X, \bmu, \bSigma, \Theta, q) =& \sum_{i=1}^n \log \left(\sum_{k = 1}^K \theta_k \normal(x_i | \bmu_k, \Sigma_k) q(z_i = k) / q(z_i = k) \right)\\
& \ge \sum_{i=1}^n \sum_{k = 1}^K q(z_i = k) \log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k) / q(z_i = k) \right)\\
& = \sum_{i=1}^n \sum_{k = 1}^K q(z_i = k) \log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right) - \sum_{i=1}^n \sum_{k=1}^K q(z_i = k) \log q(z_i = k) .
\end{align}

\begin{enumerate}


\item (5 points) You now have a lower bound objective function that depends on the Gaussian mixture model parameters $\bmu$, $\bSigma$, and $\Theta$ and the variational distribution $q$. If we hold $q$ fixed, maximizing the Gaussian mixture model parameters gets us the Gaussian EM updates (the so-called ``m-step''). You will prove this fact for one of the mixture parameters. (You are welcome to try the other parameters too for fun.) Show that maximizing the lower bound with respect to the cluster mixture probabilities $\Theta$ is exactly the EM update for these variables. 

You will need to use a Lagrange multiplier to enforce that $1 = \sum_{k=1}^K \theta_k$. Then solve for the settings of the Lagrange multiplier and each parameter $\theta_k$ to find the optimum of the constrained optimization. In other words, if the Lagrange multiplier is $\zeta$ and the Lagrangian form of the objective function is $\tilde{L}$, then you can find the solution by solving the following equations:
\begin{equation}
\begin{aligned}
\frac{\partial ~ \tilde{L}}{\partial ~ \theta_k} = 0, ~~~~~\textrm{and}~~~~~ \frac{\partial ~ \tilde{L}}{\partial ~ \zeta} = 0.
\end{aligned}
\end{equation}
Solve each equation in order and plug in the result into the original Lagrangian objective. Each zero-derivative condition will tell you something about the original objective that allows you to simplify it. 
You should end up with a final formula for the optimal value of $\theta_k$ that is relatively compact; Terms should simplify significantly to result in a simple final expression.

\solution{
Dropping all terms that aren't affected by $\Theta$ and adding a Lagrange penalty, we have the objective function
\begin{align*}
\tilde{L} = \sum_{k = 1}^K \log \theta_k \sum_{i=1}^n q(z_i = k)  + \zeta\left(1 - \sum_{k=1}^K \theta_k\right).
\end{align*}
Taking the derivative with respect to $\theta_k$ and setting to zero, we have
\begin{align*}
\partial \tilde{L} / \partial \theta_k =& \frac{1}{\theta_k} \sum_{i=1}^n q(z_i = k) - \zeta = 0\\
\theta_k =& \frac{1}{\zeta} \sum_{i=1}^n q(z_i = k)
\end{align*}
Plugging this back in, we get
\begin{align*}
\tilde{L} = \sum_{k = 1}^K \left(\log\left(\sum_{i=1}^n q(z_i = k) \right) - \log \zeta \right) \sum_{i=1}^n q(z_i = k)  + \zeta - \sum_{k=1}^K  \sum_{i=1}^n q(z_i = k).
\end{align*}
Taking derivatives with respect to $\zeta$ and setting to zero, we get
\begin{align*}
\partial \tilde{L} / \partial \zeta =& -\frac{1}{\zeta}\sum_{k = 1}^K\sum_{i=1}^n q(z_i = k)  + 1 = 0 ~,\\
\zeta =& \sum_{k = 1}^K\sum_{i=1}^n q(z_i = k) = n ~.
\end{align*}
So $\theta_k = \frac{1}{n} \sum_{i=1}^n q(z_i = k)$.

\textbf{Alternatively}, we can just directly take the partial derivative of the original Lagrangian $\tilde{L}$ for $\zeta$ and set that to zero.
\[
0 = 1 - \sum_{k = 1}^K \theta_k.
\]
Plugging in the formula for $\theta_k$ from its zero-gradient condition, we get
\begin{equation*}
\begin{aligned}
0 &= 1-  \sum_{k = 1}^K \frac{1}{\zeta} \sum_{i = 1}^n q(z_i = k)\\
1 &= \frac{1}{\zeta} \sum_{i = 1}^n \sum_{k = 1}^K q(z_i = k)
1 &= \frac{1}{\zeta} \sum_{i = 1}^n 1 \\
1 &= \frac{n}{\zeta} \\
\zeta &= n.
\end{aligned}
\end{equation*}
In the third step, we use the fact that valid $q$ distributions should sum to 1. Plugging this back into the zero-gradient condition for $\theta_k$ again gets us to the update formula. 
}

\item (5 points) This second part is a bit more involved, but follows a similar line of reasoning. Show how to find the $q$ parameters for each data point that maximize the lower bound. 

You'll need to use Lagrange multipliers to enforce that $1 = \sum_k q(z_i = k)$ for each $i$, and you should be able to consider each data point's $q$ distribution independently.
Then solve for the settings of the Lagrange multiplier and each parameter $q(z_i = k)$ to find the optimum of the constrained optimization. In other words, if the Lagrange multiplier for the $i$'th variable is $\zeta_i$ and the Lagrangian form of the objective function is $\tilde{L}$, then you can find the solution by solving the following equations:
\begin{equation}
\begin{aligned}
\frac{\partial ~ \tilde{L}}{\partial ~ q(z_i = k)} = 0, ~~~~~\textrm{and}~~~~~ \frac{\partial ~ \tilde{L}}{\partial ~ \zeta_i} = 0,
\end{aligned}
\end{equation}
where we abuse notation to refer to the multinomial probability $q(z_i = k)$ as a variable.

You should again end up with a final formula for the optimal value of $q(z_i = k)$ that is relatively compact; Terms should simplify significantly to result in a simple final expression. 


\solution{
First, we write a Lagrangian with the simplex constraint:
\begin{align*}
\tilde{L} =& \sum_{i=1}^n \sum_{k = 1}^K q(z_i = k) \log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right) - \sum_{i=1}^n \sum_{k=1}^K q(z_i = k) \log q(z_i = k) \\
&~ + \sum_{i=1}^n \zeta_i \left( 1 - \sum_{k=1}^K q(z_i = k) \right) ~.
\end{align*}
Taking the derivative with respect to $q(z_i = k)$, we get
\begin{align*}
\partial \tilde{L} / \partial q(z_i = k) & = \log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right) - \log q(z_i = k) - 1 - \zeta_i = 0 ~, \\
\log q(z_i = k) & = \log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right) - 1 - \zeta_i ~.
\end{align*}
Plugging this back in to the $\log q$ term, we get
\begin{align*}
\tilde{L} =& \sum_{i=1}^n \sum_{k = 1}^K q(z_i = k) \log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right) - \sum_{i=1}^n \sum_{k=1}^K q(z_i = k) (\log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right) - 1 - \zeta_i )  \\
&~ + \sum_{i=1}^n \zeta_i \left( 1 - \sum_{k=1}^K q(z_i = k) \right)\\
=& \sum_{i=1}^n \sum_{k = 1}^K q(z_i = k) \log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right) - \sum_{i=1}^n \sum_{k=1}^K q(z_i = k) \log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right)\\
&~ + \sum_{i=1}^n \sum_{k=1}^K q(z_i = k) (1 + \zeta_i) + \sum_{i=1}^n \zeta_i \left( 1 - \sum_{k=1}^K q(z_i = k) \right)\\
=& \sum_{i=1}^n \sum_{k=1}^K q(z_i = k) + \sum_{i=1}^n \zeta_i \sum_{k=1}^K q(x_i = k) + \sum_{i=1}^n \zeta_i  - \sum_{i=1}^n \zeta_i \sum_{k=1}^K q(z_i = k) \\
=& \sum_{i=1}^n \sum_{k=1}^K q(z_i = k) + \sum_{i=1}^n \zeta_i ~.
\end{align*}
Now that we've canceled a lot of big terms out, plugging back in for the last piece, we get
\begin{align*}
\tilde{L} =& \sum_{i=1}^n \sum_{k=1}^K \exp\left(\log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right) - 1 - \zeta_i \right) + \sum_{i=1}^n \zeta_i ~.
\end{align*}
Taking the derivative and setting to zero, we get
\begin{align*}
\partial \tilde{L} / \partial \zeta_i =& - \sum_{k=1}^K \exp\left(\log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right) - 1 - \zeta_i \right) + 1 = 0 ~, \\
1 =& \exp(-\zeta_i) \sum_{k=1}^K \exp\left(\log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right) - 1\right) ~, \\
\exp(\zeta_i)  =& \sum_{k=1}^K \exp\left(\log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right) - 1\right) ~, \\
\zeta_i  =& \log\left(\sum_{k=1}^K \exp\left(\log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right) - 1\right)\right) ~.
\end{align*}
Finally, plugging this back into the formula for $q$, we get
\begin{align*}
q(z_i = k) & = \exp(\log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right) - 1 - \zeta_i )\\
 & = \exp\left(\log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right) - 1 - \log\left(\sum_{k'=1}^{k'} \exp\left(\log\left(\theta_{k'} \normal(x_i | \bmu_{k'}, \bSigma_{k'})\right) - 1\right)\right)\right)\\
 & = \frac{\exp\left(\log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right) - 1 \right)}{\sum_{k'=1}^{k'} \exp\left(\log\left(\theta_{k'} \normal(x_i | \bmu_{k'}, \bSigma_{k'})\right) - 1 \right)}\\
 & = \frac{\exp\left(\log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right)\right) \exp\left(- 1 \right)}{\sum_{k'=1}^{k'} \exp\left(\log\left(\theta_{k'} \normal(x_i | \bmu_{k'}, \bSigma_{k'})\right)\right) \exp\left( - 1 \right)}\\
 & = \frac{\theta_k \normal(x_i | \bmu_k, \bSigma_k)}{\sum_{k'=1}^{k'} \theta_{k'} \normal(x_i | \bmu_{k'}, \bSigma_{k'})} ~.
\end{align*}

\textbf{Alternatively}, we can again directly solve for the zero-gradient condition of the Lagrange multipliers. First, recall that we found that 
\[
\log q(z_i = k) = \log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right) - 1 - \zeta_i
\]
Setting derivative to zero for each Lagrange multiplier gives us
\[
\sum_{k = 1}^K q(z_i = k) = 1.
\]
Exponentiating and plugging in the formula for $q$, we get
\begin{align*}
\sum_{k = 1}^K \exp\left(  \log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right) - 1 - \zeta_i \right) &= 1\\
\sum_{k = 1}^K \exp\left(  \log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right) - 1) \exp(-\zeta_i \right) &= 1.\\
\frac{1}{\exp(\zeta_i)} \sum_{k = 1}^K \exp\left( \log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right)\right) - 1) &= 1.\\
\sum_{k = 1}^K \exp\left( \log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right)\right) - 1) &= \exp(\zeta_i).
\end{align*}
Plugging this back into the formula for $q$, again we can simplify as on the last page.
}

\end{enumerate}

\item (5 points) Project proposal. See instructions on project homepage.

\end{enumerate}


\end{document}


